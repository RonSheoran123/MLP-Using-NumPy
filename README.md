# MLP-Using-NumPy

A simple **two-hidden-layer neural network** implemented in **pure NumPy** with:
- **ReLU** activations
- **Softmax** output
- **Cross-entropy loss**
- **Adam optimizer**
- **Optional L1 & L2 regularization**

Perfect for learning how forward propagation, backpropagation, and optimization work under the hood.

---

## Features
- Fully connected layers (Input → Hidden1 → Hidden2 → Output)
- Vectorized implementation for speed
- Adam optimizer for faster convergence
- L1 and L2 weight regularization to reduce overfitting
- No external ML libraries (only NumPy)

